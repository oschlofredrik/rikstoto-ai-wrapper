# Azure OpenAI Configuration (for GPT-4o, GPT-4o-mini, o3-mini)
# Get from Azure Portal > Azure OpenAI > Keys and Endpoint
AZURE_OPENAI_API_KEY=your-azure-openai-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2025-01-01-preview
AZURE_OPENAI_GPT4O_DEPLOYMENT=gpt-4o
AZURE_OPENAI_GPT4O_MINI_DEPLOYMENT=gpt-4o-mini
AZURE_OPENAI_O3_MINI_DEPLOYMENT=o3-mini

# Azure Mistral Configuration (for Mistral Large)
# Get from Azure AI Studio > Model-as-a-Service
AZURE_MISTRAL_ENDPOINT=https://your-mistral-endpoint.inference.ai.azure.com
AZURE_MISTRAL_API_KEY=your-mistral-api-key

# Azure Claude Configuration (for Claude 3.5 Sonnet via Databricks)
# Get from Azure Databricks > External Models
AZURE_CLAUDE_ENDPOINT=https://your-databricks-workspace.azuredatabricks.net
AZURE_CLAUDE_API_KEY=your-databricks-token

# Azure Gemini Configuration (for Gemini 1.5 Flash)
# Get from Azure API Management > Google Gemini Integration
AZURE_GEMINI_ENDPOINT=https://your-apim-gateway.azure-api.net/gemini
AZURE_GEMINI_API_KEY=your-gemini-api-key

# Legacy Hugging Face Configuration (not currently used)
HUGGINGFACE_TOKEN=hf_your_token_here
INFERENCE_ENDPOINT_URL=

# Environment Configuration
# Options: development, staging, production
ENVIRONMENT=development

# API Configuration (Optional)
# API_HOST=0.0.0.0
# API_PORT=8000

# CORS Configuration (Optional)
# Comma-separated list of allowed origins
# CORS_ORIGINS=http://localhost:3001,http://localhost:3000

# Logging Configuration (Optional)
# LOG_LEVEL=INFO
# LOG_FILE=app.log

# Model Cache Configuration (Optional)
# Uses system temp directory by default
# CACHE_DIR=/path/to/cache

# Rate Limiting (Optional - for production)
# RATE_LIMIT_ENABLED=false
# RATE_LIMIT_REQUESTS=100
# RATE_LIMIT_PERIOD=60

# Security (Optional - for production)
# SECRET_KEY=your-secret-key-here
# ENABLE_DOCS=true